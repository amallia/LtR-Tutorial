{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Efficiency/Effectiveness Trade-offs in Learning to Rank\n",
    "### Tutorial @ ICTIR 2017, HandsOn Session N. 1\n",
    "\n",
    "##### Claudio Lucchese (UniVe), Franco Maria Nardini (ISTI-CNR)\n",
    "##### High Performance Computing Lab. http://hpc.isti.cnr.it/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hpc.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from rankeval.dataset import Dataset\n",
    "from rankeval.model import RTEnsemble\n",
    "from rankeval.analysis.effectiveness import tree_wise_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "\n",
    "Given a trained LambdaMART model (stored in QuickRank format):\n",
    "\n",
    "0. setup of an experimental environment for testing different scoring methods: Conditional Operators (CondOp), VPred, QuickScorer (QS), Vectorized QuickScorer (v-QS).\n",
    "0. Execution of the different methods and comparison.\n",
    "0. Low-level analysis with ``perf`` for two of them: VPred vs QuickScorer\n",
    "0. Comparison with previously published results [QS-TOIS16].\n",
    "\n",
    "** Bonus Track **\n",
    "\n",
    "0. Multi-threaded implementation of Vectorized QuickScorer\n",
    "0. Multi-threaded scoring with RankEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0\n",
    "\n",
    "Clone and compile QuickRank. Detailed instructions on how to do it can be found at: http://quickrank.isti.cnr.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.1\n",
    "\n",
    "Clone and install RankEval. Detailed instructions on how to do it can be found at: http://rankeval.isti.cnr.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.2\n",
    "\n",
    "Download the Istella-S LETOR dataset (http://blog.istella.it/istella-learning-to-rank-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Options\n",
    "\n",
    "# paths to executable files\n",
    "QUICKRANK      = \"./quickrank/bin/quicklearn\"\n",
    "SCORER         = \"./quickrank/bin/quickscore\"\n",
    "QUICKSCORER    = \"./QuickScorer/bin/quickscorer\"\n",
    "QUICKSCORER_NS = \"./QuickScorer-noscoring/bin/quickscorer\"\n",
    "VPRED          = \"./asadi_tkde13/out/VPred\"\n",
    "VPRED_NS       = \"./asadi_tkde13-noscoring/out/VPred\"\n",
    "PERF           = \"perf\"\n",
    "\n",
    "# paths to Istella-S dataset\n",
    "train_dataset_file       = \"/data/letor-datasets/tiscali/sample/train.txt\"\n",
    "valid_dataset_file       = \"/data/letor-datasets/tiscali/sample/vali.txt\"\n",
    "test_dataset_file        = \"/data/letor-datasets/tiscali/sample/test.txt\"\n",
    "\n",
    "dataset_size = 681250\n",
    "\n",
    "# The first row of the test file used by VPred should be: \"<# rows of the file> <# features>\\n\".\n",
    "vpred_test_dataset_file  = \"/data/letor-datasets/tiscali/sample/test.vpred\"\n",
    "\n",
    "# paths to model file\n",
    "models_folder            = \"models\"\n",
    "baseline_model_file      = os.path.join(models_folder, \"istella-small.lamdamart.xml\")\n",
    "\n",
    "# setting floating point precision of Pandas\n",
    "pd.set_option('precision', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Load an existing LambdaMART model with RankEval or train it with QuickRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load a QuickRank model\n",
    "# if no model is available, use the box below to train one!\n",
    "\n",
    "baseline_model = RTEnsemble(baseline_model_file, name=\"Baseline\", format=\"QuickRank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\n",
      "      _____  _____\n",
      "     /    / /____/\n",
      "    /____\\ /    \\        QuickRank has been developed by hpc.isti.cnr.it\n",
      "    ::Quick:Rank::                             mail: quickrank@isti.cnr.it\n",
      "\u001b[0m\n",
      "\n",
      "# Ranker: LAMBDAMART\n",
      "# max no. of trees = 1000\n",
      "# no. of tree leaves = 64\n",
      "# shrinkage = 0.050000\n",
      "# min leaf support = 1\n",
      "# no. of thresholds = unlimited\n",
      "\n",
      "# Reading training dataset: /data/letor-datasets/tiscali/sample/train.txt\n",
      "#\t Reading time: 95.05 s. @ 31.69 MB/s  (post-proc.: 1.18 s.)\n",
      "#\t Dataset size: 2043304 x 220 (instances x features)\n",
      "#\t Num queries: 19245 | Avg. len: 106.173\n",
      "\n",
      "# Reading validation dataset: /data/letor-datasets/tiscali/sample/vali.txt\n",
      "#\t Reading time: 32.11 s. @ 31.46 MB/s  (post-proc.: 0.31 s.)\n",
      "#\t Dataset size: 684076 x 220 (instances x features)\n",
      "#\t Num queries: 7211 | Avg. len: 94.866\n",
      "\n",
      "#\n",
      "# Ranker: LAMBDAMART\n",
      "# max no. of trees = 1000\n",
      "# no. of tree leaves = 64\n",
      "# shrinkage = 0.050\n",
      "# min leaf support = 1\n",
      "# no. of thresholds = unlimited\n",
      "#\n",
      "# training scorer: NDCG@10\n",
      "# Initialization: 2.50 s.\n",
      "# Training:\n",
      "# -------------------------\n",
      "# iter. training validation\n",
      "# -------------------------\n",
      "      1   0.6215   0.5944 *\n",
      "      2   0.6317   0.6027 *\n",
      "      3   0.6403   0.6140 *\n",
      "      4   0.6400   0.6131\n",
      "      5   0.6397   0.6110\n",
      "      6   0.6399   0.6110\n",
      "      7   0.6405   0.6114\n",
      "      8   0.6414   0.6120\n",
      "      9   0.6420   0.6132\n",
      "     10   0.6422   0.6130\n",
      "     11   0.6423   0.6135\n",
      "     12   0.6427   0.6141 *\n",
      "     13   0.6435   0.6153 *\n",
      "     14   0.6440   0.6157 *\n",
      "     15   0.6467   0.6175 *\n",
      "     16   0.6479   0.6196 *\n",
      "     17   0.6490   0.6212 *\n",
      "     18   0.6508   0.6232 *\n",
      "     19   0.6539   0.6265 *\n",
      "     20   0.6564   0.6297 *\n",
      "     21   0.6587   0.6323 *\n",
      "     22   0.6601   0.6346 *\n",
      "     23   0.6618   0.6368 *\n",
      "     24   0.6633   0.6381 *\n",
      "     25   0.6661   0.6410 *\n",
      "     26   0.6680   0.6437 *\n",
      "     27   0.6696   0.6452 *\n",
      "     28   0.6706   0.6461 *\n",
      "     29   0.6719   0.6479 *\n",
      "     30   0.6735   0.6498 *\n",
      "     31   0.6750   0.6513 *\n",
      "     32   0.6762   0.6529 *\n",
      "     33   0.6775   0.6544 *\n",
      "     34   0.6788   0.6559 *\n",
      "     35   0.6799   0.6573 *\n",
      "     36   0.6810   0.6587 *\n",
      "     37   0.6819   0.6597 *\n",
      "     38   0.6830   0.6609 *\n",
      "     39   0.6839   0.6618 *\n",
      "     40   0.6847   0.6628 *\n",
      "     41   0.6859   0.6642 *\n",
      "     42   0.6869   0.6652 *\n",
      "     43   0.6883   0.6660 *\n",
      "     44   0.6893   0.6670 *\n",
      "     45   0.6906   0.6676 *\n",
      "     46   0.6915   0.6686 *\n",
      "     47   0.6931   0.6694 *\n",
      "     48   0.6940   0.6703 *\n",
      "     49   0.6950   0.6721 *\n",
      "     50   0.6960   0.6727 *\n",
      "     51   0.6968   0.6735 *\n",
      "     52   0.6976   0.6746 *\n",
      "     53   0.6986   0.6755 *\n",
      "     54   0.6996   0.6763 *\n",
      "     55   0.7004   0.6774 *\n",
      "     56   0.7014   0.6782 *\n",
      "     57   0.7022   0.6793 *\n",
      "     58   0.7033   0.6801 *\n",
      "     59   0.7044   0.6810 *\n",
      "     60   0.7051   0.6817 *\n",
      "     61   0.7060   0.6827 *\n",
      "     62   0.7065   0.6837 *\n",
      "     63   0.7073   0.6847 *\n",
      "     64   0.7082   0.6856 *\n",
      "     65   0.7089   0.6863 *\n",
      "     66   0.7097   0.6871 *\n",
      "     67   0.7104   0.6878 *\n",
      "     68   0.7112   0.6884 *\n",
      "     69   0.7120   0.6890 *\n",
      "     70   0.7128   0.6897 *\n",
      "     71   0.7136   0.6903 *\n",
      "     72   0.7142   0.6910 *\n",
      "     73   0.7149   0.6918 *\n",
      "     74   0.7158   0.6926 *\n",
      "     75   0.7163   0.6929 *\n",
      "     76   0.7168   0.6935 *\n",
      "     77   0.7177   0.6943 *\n",
      "     78   0.7183   0.6950 *\n",
      "     79   0.7188   0.6953 *\n",
      "     80   0.7194   0.6958 *\n",
      "     81   0.7199   0.6965 *\n",
      "     82   0.7206   0.6969 *\n",
      "     83   0.7213   0.6975 *\n",
      "     84   0.7218   0.6981 *\n",
      "     85   0.7222   0.6985 *\n",
      "     86   0.7228   0.6990 *\n",
      "     87   0.7232   0.6996 *\n",
      "     88   0.7238   0.6999 *\n",
      "     89   0.7244   0.7003 *\n",
      "     90   0.7249   0.7010 *\n",
      "     91   0.7255   0.7016 *\n",
      "     92   0.7259   0.7020 *\n",
      "     93   0.7266   0.7025 *\n",
      "     94   0.7270   0.7030 *\n",
      "     95   0.7276   0.7035 *\n",
      "     96   0.7279   0.7038 *\n",
      "     97   0.7283   0.7042 *\n",
      "     98   0.7289   0.7049 *\n",
      "     99   0.7294   0.7050 *\n",
      "    100   0.7299   0.7056 *\n",
      "    101   0.7303   0.7059 *\n",
      "    102   0.7307   0.7062 *\n",
      "    103   0.7313   0.7069 *\n",
      "    104   0.7318   0.7070 *\n",
      "    105   0.7320   0.7075 *\n",
      "    106   0.7323   0.7077 *\n",
      "    107   0.7327   0.7081 *\n",
      "    108   0.7329   0.7086 *\n",
      "    109   0.7333   0.7088 *\n",
      "    110   0.7337   0.7091 *\n",
      "    111   0.7342   0.7098 *\n",
      "    112   0.7346   0.7101 *\n",
      "    113   0.7347   0.7106 *\n",
      "    114   0.7351   0.7107 *\n",
      "    115   0.7354   0.7110 *\n",
      "    116   0.7358   0.7114 *\n",
      "    117   0.7360   0.7115 *\n",
      "    118   0.7363   0.7118 *\n",
      "    119   0.7366   0.7123 *\n",
      "    120   0.7369   0.7127 *\n",
      "    121   0.7372   0.7130 *\n",
      "    122   0.7376   0.7132 *\n",
      "    123   0.7379   0.7137 *\n",
      "    124   0.7382   0.7140 *\n",
      "    125   0.7386   0.7143 *\n",
      "    126   0.7389   0.7146 *\n",
      "    127   0.7392   0.7148 *\n",
      "    128   0.7396   0.7154 *\n",
      "    129   0.7399   0.7156 *\n",
      "    130   0.7402   0.7157 *\n",
      "    131   0.7406   0.7162 *\n",
      "    132   0.7409   0.7165 *\n",
      "    133   0.7412   0.7167 *\n",
      "    134   0.7416   0.7170 *\n",
      "    135   0.7419   0.7172 *\n",
      "    136   0.7422   0.7174 *\n",
      "    137   0.7424   0.7176 *\n",
      "    138   0.7428   0.7181 *\n",
      "    139   0.7431   0.7183 *\n",
      "    140   0.7435   0.7186 *\n",
      "    141   0.7438   0.7190 *\n",
      "    142   0.7441   0.7193 *\n",
      "    143   0.7444   0.7195 *\n",
      "    144   0.7446   0.7198 *\n",
      "    145   0.7449   0.7201 *\n",
      "    146   0.7452   0.7201\n",
      "    147   0.7455   0.7205 *\n",
      "    148   0.7458   0.7210 *\n",
      "    149   0.7460   0.7211 *\n",
      "    150   0.7464   0.7213 *\n",
      "    151   0.7465   0.7215 *\n",
      "    152   0.7468   0.7217 *\n",
      "    153   0.7470   0.7220 *\n",
      "    154   0.7473   0.7222 *\n",
      "    155   0.7476   0.7224 *\n",
      "    156   0.7478   0.7224 *\n",
      "    157   0.7480   0.7225 *\n",
      "    158   0.7482   0.7228 *\n",
      "    159   0.7483   0.7231 *\n",
      "    160   0.7486   0.7236 *\n",
      "    161   0.7488   0.7239 *\n",
      "    162   0.7491   0.7239 *\n",
      "    163   0.7492   0.7241 *\n",
      "    164   0.7494   0.7242 *\n",
      "    165   0.7495   0.7242\n",
      "    166   0.7498   0.7246 *\n",
      "    167   0.7499   0.7247 *\n",
      "    168   0.7503   0.7249 *\n",
      "    169   0.7505   0.7249\n",
      "    170   0.7507   0.7250 *\n",
      "    171   0.7508   0.7252 *\n",
      "    172   0.7511   0.7254 *\n",
      "    173   0.7513   0.7254 *\n",
      "    174   0.7516   0.7257 *\n",
      "    175   0.7517   0.7258 *\n",
      "    176   0.7520   0.7259 *\n",
      "    177   0.7522   0.7261 *\n",
      "    178   0.7524   0.7263 *\n",
      "    179   0.7526   0.7265 *\n",
      "    180   0.7527   0.7267 *\n",
      "    181   0.7529   0.7269 *\n",
      "    182   0.7531   0.7269 *\n",
      "    183   0.7534   0.7275 *\n",
      "    184   0.7536   0.7276 *\n",
      "    185   0.7539   0.7277 *\n",
      "    186   0.7540   0.7279 *\n",
      "    187   0.7543   0.7280 *\n",
      "    188   0.7546   0.7281 *\n",
      "    189   0.7548   0.7283 *\n",
      "    190   0.7549   0.7285 *\n",
      "    191   0.7550   0.7286 *\n",
      "    192   0.7553   0.7288 *\n",
      "    193   0.7555   0.7290 *\n",
      "    194   0.7556   0.7291 *\n",
      "    195   0.7557   0.7292 *\n",
      "    196   0.7559   0.7296 *\n",
      "    197   0.7561   0.7298 *\n",
      "    198   0.7562   0.7299 *\n",
      "    199   0.7565   0.7300 *\n",
      "    200   0.7566   0.7304 *\n",
      "    201   0.7568   0.7307 *\n",
      "    202   0.7570   0.7309 *\n",
      "    203   0.7571   0.7312 *\n",
      "    204   0.7573   0.7312\n",
      "    205   0.7574   0.7315 *\n",
      "    206   0.7576   0.7317 *\n",
      "    207   0.7577   0.7317 *\n",
      "    208   0.7578   0.7319 *\n",
      "    209   0.7581   0.7322 *\n",
      "    210   0.7584   0.7324 *\n",
      "    211   0.7585   0.7326 *\n",
      "    212   0.7588   0.7328 *\n",
      "    213   0.7589   0.7331 *\n",
      "    214   0.7591   0.7333 *\n",
      "    215   0.7592   0.7334 *\n",
      "    216   0.7594   0.7335 *\n",
      "    217   0.7595   0.7337 *\n",
      "    218   0.7597   0.7339 *\n",
      "    219   0.7599   0.7340 *\n",
      "    220   0.7601   0.7341 *\n",
      "    221   0.7602   0.7343 *\n",
      "    222   0.7603   0.7344 *\n",
      "    223   0.7604   0.7345 *\n",
      "    224   0.7606   0.7346 *\n",
      "    225   0.7607   0.7348 *\n",
      "    226   0.7609   0.7347\n",
      "    227   0.7610   0.7350 *\n",
      "    228   0.7611   0.7352 *\n",
      "    229   0.7612   0.7354 *\n",
      "    230   0.7615   0.7358 *\n",
      "    231   0.7616   0.7359 *\n",
      "    232   0.7616   0.7361 *\n",
      "    233   0.7618   0.7361 *\n",
      "    234   0.7619   0.7363 *\n",
      "    235   0.7620   0.7364 *\n",
      "    236   0.7622   0.7365 *\n",
      "    237   0.7624   0.7366 *\n",
      "    238   0.7625   0.7367 *\n",
      "    239   0.7627   0.7369 *\n",
      "    240   0.7629   0.7372 *\n",
      "    241   0.7630   0.7372 *\n",
      "    242   0.7632   0.7374 *\n",
      "    243   0.7632   0.7374 *\n",
      "    244   0.7633   0.7375 *\n",
      "    245   0.7635   0.7376 *\n",
      "    246   0.7635   0.7376 *\n",
      "    247   0.7637   0.7378 *\n",
      "    248   0.7639   0.7377\n",
      "    249   0.7641   0.7377\n",
      "    250   0.7641   0.7379 *\n",
      "    251   0.7643   0.7379 *\n",
      "    252   0.7643   0.7377\n",
      "    253   0.7645   0.7379 *\n",
      "    254   0.7645   0.7381 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    255   0.7646   0.7380\n",
      "    256   0.7648   0.7381\n",
      "    257   0.7649   0.7383 *\n",
      "    258   0.7650   0.7382\n",
      "    259   0.7652   0.7384 *\n",
      "    260   0.7653   0.7384\n",
      "    261   0.7654   0.7387 *\n",
      "    262   0.7655   0.7388 *\n",
      "    263   0.7656   0.7388\n",
      "    264   0.7658   0.7389 *\n",
      "    265   0.7659   0.7388\n",
      "    266   0.7660   0.7388\n",
      "    267   0.7663   0.7393 *\n",
      "    268   0.7664   0.7394 *\n",
      "    269   0.7664   0.7394\n",
      "    270   0.7665   0.7393\n",
      "    271   0.7666   0.7395 *\n",
      "    272   0.7667   0.7396 *\n",
      "    273   0.7668   0.7396 *\n",
      "    274   0.7669   0.7398 *\n",
      "    275   0.7670   0.7399 *\n",
      "    276   0.7672   0.7400 *\n",
      "    277   0.7673   0.7402 *\n",
      "    278   0.7674   0.7404 *\n",
      "    279   0.7675   0.7404 *\n",
      "    280   0.7676   0.7404 *\n",
      "    281   0.7677   0.7403\n",
      "    282   0.7677   0.7403\n",
      "    283   0.7678   0.7405 *\n",
      "    284   0.7679   0.7406 *\n",
      "    285   0.7682   0.7407 *\n",
      "    286   0.7683   0.7406\n",
      "    287   0.7684   0.7406\n",
      "    288   0.7685   0.7408 *\n",
      "    289   0.7686   0.7409 *\n",
      "    290   0.7687   0.7411 *\n",
      "    291   0.7687   0.7411\n",
      "    292   0.7688   0.7410\n",
      "    293   0.7689   0.7412 *\n",
      "    294   0.7691   0.7412 *\n",
      "    295   0.7692   0.7415 *\n",
      "    296   0.7693   0.7415 *\n",
      "    297   0.7693   0.7417 *\n",
      "    298   0.7693   0.7418 *\n",
      "    299   0.7695   0.7419 *\n",
      "    300   0.7695   0.7421 *\n",
      "    301   0.7697   0.7421 *\n",
      "    302   0.7698   0.7423 *\n",
      "    303   0.7699   0.7423 *\n",
      "    304   0.7701   0.7424 *\n",
      "    305   0.7701   0.7424 *\n",
      "    306   0.7702   0.7426 *\n",
      "    307   0.7703   0.7426 *\n",
      "    308   0.7704   0.7426\n",
      "    309   0.7704   0.7426\n",
      "    310   0.7706   0.7426\n",
      "    311   0.7706   0.7426\n",
      "    312   0.7708   0.7428 *\n",
      "    313   0.7709   0.7429 *\n",
      "    314   0.7710   0.7430 *\n",
      "    315   0.7712   0.7432 *\n",
      "    316   0.7712   0.7432 *\n",
      "    317   0.7713   0.7432\n",
      "    318   0.7714   0.7433 *\n",
      "    319   0.7715   0.7435 *\n",
      "    320   0.7716   0.7434\n",
      "    321   0.7717   0.7435 *\n",
      "    322   0.7717   0.7435\n",
      "    323   0.7719   0.7436 *\n",
      "    324   0.7719   0.7438 *\n",
      "    325   0.7721   0.7439 *\n",
      "    326   0.7721   0.7439\n",
      "    327   0.7722   0.7440 *\n",
      "    328   0.7723   0.7440\n",
      "    329   0.7724   0.7440\n",
      "    330   0.7725   0.7441 *\n",
      "    331   0.7726   0.7442 *\n",
      "    332   0.7727   0.7441\n",
      "    333   0.7729   0.7442 *\n",
      "    334   0.7730   0.7442 *\n",
      "    335   0.7731   0.7443 *\n",
      "    336   0.7732   0.7443 *\n",
      "    337   0.7733   0.7443 *\n",
      "    338   0.7733   0.7443 *\n",
      "    339   0.7733   0.7445 *\n",
      "    340   0.7734   0.7446 *\n",
      "    341   0.7735   0.7445\n",
      "    342   0.7736   0.7447 *\n",
      "    343   0.7737   0.7448 *\n",
      "    344   0.7738   0.7448 *\n",
      "    345   0.7739   0.7450 *\n",
      "    346   0.7740   0.7449\n",
      "    347   0.7740   0.7450\n",
      "    348   0.7741   0.7450 *\n",
      "    349   0.7743   0.7450 *\n",
      "    350   0.7744   0.7452 *\n",
      "    351   0.7745   0.7453 *\n",
      "    352   0.7746   0.7453 *\n",
      "    353   0.7747   0.7453\n",
      "    354   0.7747   0.7454 *\n",
      "    355   0.7747   0.7453\n",
      "    356   0.7748   0.7455 *\n",
      "    357   0.7749   0.7454\n",
      "    358   0.7750   0.7456 *\n",
      "    359   0.7750   0.7457 *\n",
      "    360   0.7750   0.7456\n",
      "    361   0.7752   0.7459 *\n",
      "    362   0.7753   0.7458\n",
      "    363   0.7755   0.7458\n",
      "    364   0.7756   0.7459 *\n",
      "    365   0.7757   0.7460 *\n",
      "    366   0.7757   0.7460 *\n",
      "    367   0.7759   0.7462 *\n",
      "    368   0.7759   0.7463 *\n",
      "    369   0.7761   0.7464 *\n",
      "    370   0.7762   0.7464\n",
      "    371   0.7763   0.7462\n",
      "    372   0.7764   0.7463\n",
      "    373   0.7764   0.7464 *\n",
      "    374   0.7765   0.7463\n",
      "    375   0.7765   0.7464 *\n",
      "    376   0.7766   0.7464\n",
      "    377   0.7766   0.7464\n",
      "    378   0.7767   0.7463\n",
      "    379   0.7767   0.7463\n",
      "    380   0.7768   0.7465 *\n",
      "    381   0.7769   0.7467 *\n",
      "    382   0.7770   0.7468 *\n",
      "    383   0.7770   0.7467\n",
      "    384   0.7771   0.7467\n",
      "    385   0.7771   0.7468 *\n",
      "    386   0.7772   0.7469 *\n",
      "    387   0.7773   0.7469\n",
      "    388   0.7774   0.7469\n",
      "    389   0.7775   0.7470 *\n",
      "    390   0.7776   0.7470 *\n",
      "    391   0.7777   0.7471 *\n",
      "    392   0.7777   0.7471\n",
      "    393   0.7777   0.7472 *\n",
      "    394   0.7778   0.7472\n",
      "    395   0.7780   0.7474 *\n",
      "    396   0.7781   0.7472\n",
      "    397   0.7782   0.7473\n",
      "    398   0.7783   0.7474 *\n",
      "    399   0.7783   0.7475 *\n",
      "    400   0.7785   0.7475 *\n",
      "    401   0.7785   0.7476 *\n",
      "    402   0.7786   0.7477 *\n",
      "    403   0.7787   0.7477 *\n",
      "    404   0.7787   0.7477 *\n",
      "    405   0.7787   0.7479 *\n",
      "    406   0.7788   0.7480 *\n",
      "    407   0.7788   0.7480\n",
      "    408   0.7788   0.7481 *\n",
      "    409   0.7789   0.7481 *\n",
      "    410   0.7790   0.7482 *\n",
      "    411   0.7791   0.7482 *\n",
      "    412   0.7792   0.7483 *\n",
      "    413   0.7793   0.7483\n",
      "    414   0.7794   0.7482\n",
      "    415   0.7795   0.7482\n",
      "    416   0.7796   0.7483\n",
      "    417   0.7796   0.7483 *\n",
      "    418   0.7796   0.7484 *\n",
      "    419   0.7798   0.7484\n",
      "    420   0.7799   0.7485 *\n",
      "    421   0.7800   0.7485\n",
      "    422   0.7800   0.7486 *\n",
      "    423   0.7801   0.7486 *\n",
      "    424   0.7802   0.7487 *\n",
      "    425   0.7803   0.7486\n",
      "    426   0.7804   0.7487\n",
      "    427   0.7804   0.7489 *\n",
      "    428   0.7805   0.7489 *\n",
      "    429   0.7805   0.7490 *\n",
      "    430   0.7806   0.7491 *\n",
      "    431   0.7806   0.7491\n",
      "    432   0.7807   0.7492 *\n",
      "    433   0.7808   0.7492\n",
      "    434   0.7809   0.7493 *\n",
      "    435   0.7809   0.7493 *\n",
      "    436   0.7811   0.7494 *\n",
      "    437   0.7810   0.7494 *\n",
      "    438   0.7811   0.7494\n",
      "    439   0.7812   0.7492\n",
      "    440   0.7813   0.7494\n",
      "    441   0.7813   0.7496 *\n",
      "    442   0.7814   0.7495\n",
      "    443   0.7814   0.7496\n",
      "    444   0.7815   0.7496\n",
      "    445   0.7815   0.7495\n",
      "    446   0.7816   0.7496\n",
      "    447   0.7816   0.7496\n",
      "    448   0.7817   0.7496 *\n",
      "    449   0.7818   0.7496\n",
      "    450   0.7818   0.7497 *\n",
      "    451   0.7819   0.7497 *\n",
      "    452   0.7819   0.7498 *\n",
      "    453   0.7820   0.7499 *\n",
      "    454   0.7820   0.7499 *\n",
      "    455   0.7821   0.7500 *\n",
      "    456   0.7821   0.7500\n",
      "    457   0.7823   0.7500 *\n",
      "    458   0.7824   0.7500 *\n",
      "    459   0.7824   0.7499\n",
      "    460   0.7825   0.7501 *\n",
      "    461   0.7825   0.7501 *\n",
      "    462   0.7826   0.7501\n",
      "    463   0.7826   0.7500\n",
      "    464   0.7827   0.7501\n",
      "    465   0.7828   0.7500\n",
      "    466   0.7829   0.7501\n",
      "    467   0.7831   0.7502 *\n",
      "    468   0.7831   0.7501\n",
      "    469   0.7831   0.7502 *\n",
      "    470   0.7833   0.7501\n",
      "    471   0.7833   0.7504 *\n",
      "    472   0.7833   0.7503\n",
      "    473   0.7834   0.7504 *\n",
      "    474   0.7834   0.7504\n",
      "    475   0.7834   0.7505 *\n",
      "    476   0.7835   0.7505\n",
      "    477   0.7836   0.7504\n",
      "    478   0.7836   0.7504\n",
      "    479   0.7837   0.7505\n",
      "    480   0.7838   0.7506 *\n",
      "    481   0.7838   0.7506\n",
      "    482   0.7838   0.7507 *\n",
      "    483   0.7839   0.7507 *\n",
      "    484   0.7839   0.7507\n",
      "    485   0.7840   0.7507\n",
      "    486   0.7840   0.7507\n",
      "    487   0.7841   0.7508 *\n",
      "    488   0.7842   0.7507\n",
      "    489   0.7842   0.7509 *\n",
      "    490   0.7843   0.7509\n",
      "    491   0.7844   0.7510 *\n",
      "    492   0.7845   0.7510\n",
      "    493   0.7846   0.7511 *\n",
      "    494   0.7847   0.7510\n",
      "    495   0.7847   0.7510\n",
      "    496   0.7848   0.7512 *\n",
      "    497   0.7848   0.7513 *\n",
      "    498   0.7848   0.7514 *\n",
      "    499   0.7850   0.7513\n",
      "    500   0.7850   0.7513\n",
      "    501   0.7850   0.7514\n",
      "    502   0.7851   0.7514 *\n",
      "    503   0.7851   0.7514\n",
      "    504   0.7852   0.7514 *\n",
      "    505   0.7852   0.7515 *\n",
      "    506   0.7853   0.7516 *\n",
      "    507   0.7854   0.7516 *\n",
      "    508   0.7855   0.7516\n",
      "    509   0.7855   0.7516\n",
      "    510   0.7855   0.7516 *\n",
      "    511   0.7856   0.7516\n",
      "    512   0.7857   0.7517 *\n",
      "    513   0.7857   0.7517\n",
      "    514   0.7857   0.7516\n",
      "    515   0.7858   0.7516\n",
      "    516   0.7859   0.7516\n",
      "    517   0.7859   0.7516\n",
      "    518   0.7860   0.7516\n",
      "    519   0.7861   0.7515\n",
      "    520   0.7860   0.7516\n",
      "    521   0.7861   0.7516\n",
      "    522   0.7861   0.7517\n",
      "    523   0.7862   0.7517\n",
      "    524   0.7863   0.7517\n",
      "    525   0.7863   0.7517\n",
      "    526   0.7864   0.7516\n",
      "    527   0.7864   0.7517\n",
      "    528   0.7865   0.7517 *\n",
      "    529   0.7865   0.7517\n",
      "    530   0.7865   0.7518 *\n",
      "    531   0.7866   0.7518 *\n",
      "    532   0.7867   0.7519 *\n",
      "    533   0.7867   0.7519 *\n",
      "    534   0.7867   0.7519\n",
      "    535   0.7868   0.7522 *\n",
      "    536   0.7868   0.7521\n",
      "    537   0.7868   0.7522 *\n",
      "    538   0.7868   0.7522 *\n",
      "    539   0.7869   0.7524 *\n",
      "    540   0.7871   0.7524 *\n",
      "    541   0.7871   0.7523\n",
      "    542   0.7872   0.7525 *\n",
      "    543   0.7872   0.7526 *\n",
      "    544   0.7873   0.7526 *\n",
      "    545   0.7873   0.7525\n",
      "    546   0.7874   0.7526\n",
      "    547   0.7874   0.7526 *\n",
      "    548   0.7875   0.7527 *\n",
      "    549   0.7876   0.7527 *\n",
      "    550   0.7877   0.7527 *\n",
      "    551   0.7878   0.7527\n",
      "    552   0.7878   0.7526\n",
      "    553   0.7879   0.7528 *\n",
      "    554   0.7879   0.7527\n",
      "    555   0.7880   0.7528 *\n",
      "    556   0.7881   0.7529 *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    557   0.7881   0.7528\n",
      "    558   0.7882   0.7528\n",
      "    559   0.7882   0.7528\n",
      "    560   0.7882   0.7527\n",
      "    561   0.7883   0.7528\n",
      "    562   0.7883   0.7528\n",
      "    563   0.7884   0.7527\n",
      "    564   0.7884   0.7527\n",
      "    565   0.7885   0.7528\n",
      "    566   0.7886   0.7528\n",
      "    567   0.7887   0.7529\n",
      "    568   0.7888   0.7529\n",
      "    569   0.7889   0.7530 *\n",
      "    570   0.7889   0.7530 *\n",
      "    571   0.7890   0.7530 *\n",
      "    572   0.7889   0.7530 *\n",
      "    573   0.7890   0.7530\n",
      "    574   0.7890   0.7530\n",
      "    575   0.7890   0.7532 *\n",
      "    576   0.7891   0.7531\n",
      "    577   0.7891   0.7532\n",
      "    578   0.7892   0.7531\n",
      "    579   0.7892   0.7531\n",
      "    580   0.7893   0.7533 *\n",
      "    581   0.7893   0.7534 *\n",
      "    582   0.7893   0.7533\n",
      "    583   0.7894   0.7534 *\n",
      "    584   0.7894   0.7533\n",
      "    585   0.7895   0.7533\n",
      "    586   0.7896   0.7533\n",
      "    587   0.7896   0.7534 *\n",
      "    588   0.7897   0.7535 *\n",
      "    589   0.7897   0.7535 *\n",
      "    590   0.7898   0.7535\n",
      "    591   0.7898   0.7536 *\n",
      "    592   0.7899   0.7535\n",
      "    593   0.7899   0.7535\n",
      "    594   0.7898   0.7536\n",
      "    595   0.7899   0.7535\n",
      "    596   0.7900   0.7538 *\n",
      "    597   0.7900   0.7537\n",
      "    598   0.7900   0.7537\n",
      "    599   0.7901   0.7538 *\n",
      "    600   0.7901   0.7539 *\n",
      "    601   0.7902   0.7538\n",
      "    602   0.7902   0.7539 *\n",
      "    603   0.7902   0.7540 *\n",
      "    604   0.7902   0.7540\n",
      "    605   0.7904   0.7540 *\n",
      "    606   0.7904   0.7540\n",
      "    607   0.7904   0.7540\n",
      "    608   0.7904   0.7540\n",
      "    609   0.7905   0.7539\n",
      "    610   0.7906   0.7540\n",
      "    611   0.7906   0.7539\n",
      "    612   0.7906   0.7539\n",
      "    613   0.7907   0.7540\n",
      "    614   0.7908   0.7540\n",
      "    615   0.7908   0.7540\n",
      "    616   0.7908   0.7541 *\n",
      "    617   0.7908   0.7541\n",
      "    618   0.7909   0.7541 *\n",
      "    619   0.7910   0.7541\n",
      "    620   0.7910   0.7541\n",
      "    621   0.7911   0.7541\n",
      "    622   0.7911   0.7541 *\n",
      "    623   0.7912   0.7540\n",
      "    624   0.7913   0.7540\n",
      "    625   0.7914   0.7539\n",
      "    626   0.7914   0.7540\n",
      "    627   0.7914   0.7541\n",
      "    628   0.7915   0.7541\n",
      "    629   0.7915   0.7541\n",
      "    630   0.7915   0.7542 *\n",
      "    631   0.7915   0.7541\n",
      "    632   0.7915   0.7540\n",
      "    633   0.7917   0.7541\n",
      "    634   0.7917   0.7541\n",
      "    635   0.7917   0.7541\n",
      "    636   0.7918   0.7542\n",
      "    637   0.7918   0.7540\n",
      "    638   0.7918   0.7540\n",
      "    639   0.7919   0.7540\n",
      "    640   0.7919   0.7541\n",
      "    641   0.7920   0.7542 *\n",
      "    642   0.7920   0.7542 *\n",
      "    643   0.7921   0.7543 *\n",
      "    644   0.7921   0.7544 *\n",
      "    645   0.7921   0.7544\n",
      "    646   0.7922   0.7545 *\n",
      "    647   0.7923   0.7545 *\n",
      "    648   0.7923   0.7546 *\n",
      "    649   0.7923   0.7545\n",
      "    650   0.7924   0.7545\n",
      "    651   0.7924   0.7545\n",
      "    652   0.7925   0.7545\n",
      "    653   0.7925   0.7546\n",
      "    654   0.7926   0.7545\n",
      "    655   0.7926   0.7545\n",
      "    656   0.7926   0.7545\n",
      "    657   0.7927   0.7545\n",
      "    658   0.7927   0.7545\n",
      "    659   0.7927   0.7546\n",
      "    660   0.7928   0.7545\n",
      "    661   0.7928   0.7546 *\n",
      "    662   0.7929   0.7546\n",
      "    663   0.7929   0.7546\n",
      "    664   0.7930   0.7546 *\n",
      "    665   0.7929   0.7547 *\n",
      "    666   0.7929   0.7547\n",
      "    667   0.7931   0.7547\n",
      "    668   0.7931   0.7547\n",
      "    669   0.7931   0.7548 *\n",
      "    670   0.7932   0.7548 *\n",
      "    671   0.7932   0.7549 *\n",
      "    672   0.7933   0.7548\n",
      "    673   0.7933   0.7549 *\n",
      "    674   0.7934   0.7548\n",
      "    675   0.7934   0.7549 *\n",
      "    676   0.7934   0.7550 *\n",
      "    677   0.7935   0.7550 *\n",
      "    678   0.7935   0.7550\n",
      "    679   0.7936   0.7550\n",
      "    680   0.7936   0.7550\n",
      "    681   0.7936   0.7550 *\n",
      "    682   0.7936   0.7550\n",
      "    683   0.7937   0.7550\n",
      "    684   0.7937   0.7550\n",
      "    685   0.7937   0.7550\n",
      "    686   0.7937   0.7550\n",
      "    687   0.7937   0.7550\n",
      "    688   0.7938   0.7550\n",
      "    689   0.7938   0.7550\n",
      "    690   0.7938   0.7552 *\n",
      "    691   0.7939   0.7553 *\n",
      "    692   0.7939   0.7552\n",
      "    693   0.7939   0.7552\n",
      "    694   0.7940   0.7552\n",
      "    695   0.7941   0.7552\n",
      "    696   0.7941   0.7554 *\n",
      "    697   0.7942   0.7553\n",
      "    698   0.7942   0.7553\n",
      "    699   0.7942   0.7552\n",
      "    700   0.7943   0.7553\n",
      "    701   0.7943   0.7553\n",
      "    702   0.7944   0.7553\n",
      "    703   0.7944   0.7553\n",
      "    704   0.7944   0.7552\n",
      "    705   0.7945   0.7552\n",
      "    706   0.7945   0.7553\n",
      "    707   0.7945   0.7553\n",
      "    708   0.7945   0.7554\n",
      "    709   0.7946   0.7554\n",
      "    710   0.7947   0.7554 *\n",
      "    711   0.7947   0.7554\n",
      "    712   0.7948   0.7555 *\n",
      "    713   0.7948   0.7556 *\n",
      "    714   0.7948   0.7557 *\n",
      "    715   0.7948   0.7557\n",
      "    716   0.7949   0.7557 *\n",
      "    717   0.7949   0.7556\n",
      "    718   0.7950   0.7556\n",
      "    719   0.7950   0.7556\n",
      "    720   0.7950   0.7556\n",
      "    721   0.7951   0.7556\n",
      "    722   0.7951   0.7557 *\n",
      "    723   0.7952   0.7557\n",
      "    724   0.7952   0.7557\n",
      "    725   0.7953   0.7557\n",
      "    726   0.7953   0.7557\n",
      "    727   0.7953   0.7557\n",
      "    728   0.7953   0.7558 *\n",
      "    729   0.7954   0.7558 *\n",
      "    730   0.7955   0.7558 *\n",
      "    731   0.7955   0.7558 *\n",
      "    732   0.7955   0.7558\n",
      "    733   0.7956   0.7558 *\n",
      "    734   0.7956   0.7559 *\n",
      "    735   0.7956   0.7559 *\n",
      "    736   0.7957   0.7559\n",
      "    737   0.7957   0.7557\n",
      "    738   0.7958   0.7557\n",
      "    739   0.7958   0.7558\n",
      "    740   0.7958   0.7558\n",
      "    741   0.7958   0.7560 *\n",
      "    742   0.7959   0.7560\n",
      "    743   0.7960   0.7560 *\n",
      "    744   0.7960   0.7560 *\n",
      "    745   0.7961   0.7561 *\n",
      "    746   0.7961   0.7561\n",
      "    747   0.7961   0.7561 *\n",
      "    748   0.7961   0.7561\n",
      "    749   0.7962   0.7561\n",
      "    750   0.7963   0.7561 *\n",
      "    751   0.7963   0.7561\n",
      "    752   0.7963   0.7560\n",
      "    753   0.7964   0.7560\n",
      "    754   0.7964   0.7560\n",
      "    755   0.7965   0.7562 *\n",
      "    756   0.7964   0.7562 *\n",
      "    757   0.7965   0.7561\n",
      "    758   0.7966   0.7561\n",
      "    759   0.7966   0.7561\n",
      "    760   0.7966   0.7561\n",
      "    761   0.7966   0.7561\n",
      "    762   0.7967   0.7562\n",
      "    763   0.7967   0.7562 *\n",
      "    764   0.7968   0.7562 *\n",
      "    765   0.7968   0.7562\n",
      "    766   0.7968   0.7563 *\n",
      "    767   0.7969   0.7563 *\n",
      "    768   0.7970   0.7565 *\n",
      "    769   0.7970   0.7564\n",
      "    770   0.7971   0.7564\n",
      "    771   0.7971   0.7564\n",
      "    772   0.7971   0.7564\n",
      "    773   0.7971   0.7564\n",
      "    774   0.7972   0.7565 *\n",
      "    775   0.7972   0.7565 *\n",
      "    776   0.7973   0.7565 *\n",
      "    777   0.7973   0.7565\n",
      "    778   0.7974   0.7565 *\n",
      "    779   0.7975   0.7565 *\n",
      "    780   0.7974   0.7565\n",
      "    781   0.7975   0.7564\n",
      "    782   0.7975   0.7565\n",
      "    783   0.7976   0.7565\n",
      "    784   0.7976   0.7566 *\n",
      "    785   0.7977   0.7567 *\n",
      "    786   0.7977   0.7566\n",
      "    787   0.7977   0.7567\n",
      "    788   0.7977   0.7567 *\n",
      "    789   0.7978   0.7567 *\n",
      "    790   0.7978   0.7567 *\n",
      "    791   0.7979   0.7567\n",
      "    792   0.7979   0.7567\n",
      "    793   0.7979   0.7568 *\n",
      "    794   0.7980   0.7568 *\n",
      "    795   0.7980   0.7567\n",
      "    796   0.7981   0.7567\n",
      "    797   0.7981   0.7568 *\n",
      "    798   0.7981   0.7569 *\n",
      "    799   0.7981   0.7568\n",
      "    800   0.7982   0.7568\n",
      "    801   0.7982   0.7568\n",
      "    802   0.7983   0.7568\n",
      "    803   0.7983   0.7568\n",
      "    804   0.7983   0.7568\n",
      "    805   0.7984   0.7569\n",
      "    806   0.7985   0.7569 *\n",
      "    807   0.7985   0.7570 *\n",
      "    808   0.7985   0.7569\n",
      "    809   0.7986   0.7571 *\n",
      "    810   0.7986   0.7570\n",
      "    811   0.7986   0.7570\n",
      "    812   0.7987   0.7569\n",
      "    813   0.7987   0.7568\n",
      "    814   0.7987   0.7570\n",
      "    815   0.7988   0.7569\n",
      "    816   0.7988   0.7569\n",
      "    817   0.7989   0.7570\n",
      "    818   0.7989   0.7570\n",
      "    819   0.7990   0.7570\n",
      "    820   0.7990   0.7570\n",
      "    821   0.7990   0.7570\n",
      "    822   0.7990   0.7571\n",
      "    823   0.7990   0.7571 *\n",
      "    824   0.7991   0.7571\n",
      "    825   0.7992   0.7571\n",
      "    826   0.7992   0.7572 *\n",
      "    827   0.7993   0.7572 *\n",
      "    828   0.7992   0.7570\n",
      "    829   0.7993   0.7571\n",
      "    830   0.7994   0.7571\n",
      "    831   0.7994   0.7570\n",
      "    832   0.7994   0.7571\n",
      "    833   0.7995   0.7572\n",
      "    834   0.7996   0.7572\n",
      "    835   0.7996   0.7572\n",
      "    836   0.7996   0.7572\n",
      "    837   0.7996   0.7571\n",
      "    838   0.7996   0.7572\n",
      "    839   0.7997   0.7572 *\n",
      "    840   0.7998   0.7573 *\n",
      "    841   0.7998   0.7573\n",
      "    842   0.7999   0.7573\n",
      "    843   0.7999   0.7573\n",
      "    844   0.7999   0.7573\n",
      "    845   0.7999   0.7574 *\n",
      "    846   0.8000   0.7575 *\n",
      "    847   0.8000   0.7575 *\n",
      "    848   0.8001   0.7575\n",
      "    849   0.8002   0.7575 *\n",
      "    850   0.8002   0.7574\n",
      "    851   0.8003   0.7575\n",
      "    852   0.8003   0.7575 *\n",
      "    853   0.8003   0.7575\n",
      "    854   0.8004   0.7575 *\n",
      "    855   0.8004   0.7575\n",
      "    856   0.8004   0.7576 *\n",
      "    857   0.8005   0.7576 *\n",
      "    858   0.8005   0.7576\n",
      "    859   0.8006   0.7576 *\n",
      "    860   0.8006   0.7576\n",
      "    861   0.8007   0.7575\n",
      "    862   0.8007   0.7575\n",
      "    863   0.8007   0.7575\n",
      "    864   0.8007   0.7574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    865   0.8008   0.7574\n",
      "    866   0.8009   0.7574\n",
      "    867   0.8009   0.7574\n",
      "    868   0.8009   0.7574\n",
      "    869   0.8009   0.7575\n",
      "    870   0.8010   0.7576\n",
      "    871   0.8009   0.7575\n",
      "    872   0.8010   0.7575\n",
      "    873   0.8011   0.7575\n",
      "    874   0.8011   0.7575\n",
      "    875   0.8012   0.7575\n",
      "    876   0.8012   0.7575\n",
      "    877   0.8013   0.7576\n",
      "    878   0.8013   0.7576\n",
      "    879   0.8013   0.7575\n",
      "    880   0.8014   0.7575\n",
      "    881   0.8014   0.7575\n",
      "    882   0.8014   0.7575\n",
      "    883   0.8014   0.7576 *\n",
      "    884   0.8015   0.7576\n",
      "    885   0.8015   0.7576\n",
      "    886   0.8016   0.7576\n",
      "    887   0.8016   0.7575\n",
      "    888   0.8016   0.7577 *\n",
      "    889   0.8016   0.7576\n",
      "    890   0.8017   0.7576\n",
      "    891   0.8018   0.7576\n",
      "    892   0.8018   0.7576\n",
      "    893   0.8018   0.7576\n",
      "    894   0.8018   0.7576\n",
      "    895   0.8019   0.7577 *\n",
      "    896   0.8019   0.7577 *\n",
      "    897   0.8020   0.7577 *\n",
      "    898   0.8021   0.7578 *\n",
      "    899   0.8021   0.7578\n",
      "    900   0.8022   0.7578 *\n",
      "    901   0.8022   0.7578\n",
      "    902   0.8022   0.7578\n",
      "    903   0.8022   0.7578\n",
      "    904   0.8023   0.7578\n",
      "    905   0.8023   0.7578\n",
      "    906   0.8023   0.7578\n",
      "    907   0.8024   0.7578\n",
      "    908   0.8024   0.7578 *\n",
      "    909   0.8024   0.7579 *\n",
      "    910   0.8025   0.7579 *\n",
      "    911   0.8026   0.7579 *\n",
      "    912   0.8026   0.7579 *\n",
      "    913   0.8026   0.7579 *\n",
      "    914   0.8027   0.7578\n",
      "    915   0.8027   0.7579\n",
      "    916   0.8027   0.7578\n",
      "    917   0.8027   0.7578\n",
      "    918   0.8028   0.7577\n",
      "    919   0.8028   0.7577\n",
      "    920   0.8029   0.7576\n",
      "    921   0.8029   0.7577\n",
      "    922   0.8029   0.7577\n",
      "    923   0.8030   0.7579\n",
      "    924   0.8030   0.7578\n",
      "    925   0.8030   0.7579\n",
      "    926   0.8030   0.7578\n",
      "    927   0.8031   0.7580 *\n",
      "    928   0.8031   0.7580 *\n",
      "    929   0.8031   0.7580 *\n",
      "    930   0.8032   0.7580\n",
      "    931   0.8032   0.7580\n",
      "    932   0.8032   0.7580\n",
      "    933   0.8032   0.7580 *\n",
      "    934   0.8033   0.7581 *\n",
      "    935   0.8034   0.7581\n",
      "    936   0.8034   0.7581 *\n",
      "    937   0.8034   0.7581 *\n",
      "    938   0.8034   0.7580\n",
      "    939   0.8035   0.7581\n",
      "    940   0.8035   0.7582 *\n",
      "    941   0.8035   0.7582 *\n",
      "    942   0.8036   0.7583 *\n",
      "    943   0.8036   0.7583 *\n",
      "    944   0.8037   0.7583\n",
      "    945   0.8038   0.7583\n",
      "    946   0.8038   0.7585 *\n",
      "    947   0.8038   0.7585 *\n",
      "    948   0.8038   0.7585\n",
      "    949   0.8039   0.7584\n",
      "    950   0.8039   0.7584\n",
      "    951   0.8039   0.7584\n",
      "    952   0.8040   0.7584\n",
      "    953   0.8040   0.7583\n",
      "    954   0.8040   0.7583\n",
      "    955   0.8041   0.7583\n",
      "    956   0.8041   0.7584\n",
      "    957   0.8042   0.7584\n",
      "    958   0.8041   0.7584\n",
      "    959   0.8041   0.7584\n",
      "    960   0.8042   0.7584\n",
      "    961   0.8042   0.7584\n",
      "    962   0.8042   0.7584\n",
      "    963   0.8043   0.7585\n",
      "    964   0.8043   0.7585 *\n",
      "    965   0.8043   0.7585 *\n",
      "    966   0.8044   0.7585\n",
      "    967   0.8044   0.7585\n",
      "    968   0.8044   0.7585 *\n",
      "    969   0.8044   0.7585\n",
      "    970   0.8045   0.7585\n",
      "    971   0.8045   0.7585 *\n",
      "    972   0.8045   0.7584\n",
      "    973   0.8045   0.7586 *\n",
      "    974   0.8045   0.7585\n",
      "    975   0.8046   0.7585\n",
      "    976   0.8046   0.7584\n",
      "    977   0.8046   0.7586\n",
      "    978   0.8047   0.7585\n",
      "    979   0.8047   0.7585\n",
      "    980   0.8048   0.7585\n",
      "    981   0.8049   0.7584\n",
      "    982   0.8049   0.7584\n",
      "    983   0.8049   0.7585\n",
      "    984   0.8048   0.7585\n",
      "    985   0.8049   0.7585\n",
      "    986   0.8049   0.7585\n",
      "    987   0.8049   0.7584\n",
      "    988   0.8050   0.7583\n",
      "    989   0.8050   0.7583\n",
      "    990   0.8050   0.7584\n",
      "    991   0.8051   0.7584\n",
      "    992   0.8051   0.7583\n",
      "    993   0.8051   0.7584\n",
      "    994   0.8051   0.7584\n",
      "    995   0.8051   0.7584\n",
      "    996   0.8052   0.7584\n",
      "    997   0.8053   0.7585\n",
      "    998   0.8053   0.7585\n",
      "    999   0.8052   0.7586 *\n",
      "   1000   0.8053   0.7586 *\n",
      "\n",
      "NDCG@10 on training data = 0.8053\n",
      "NDCG@10 on validation data = 0.7586\n",
      "\n",
      "#\t Training Time: 10326.52 s.\n",
      "\n",
      "# Writing model to file: /home/letortutorial/quickrank.1000T.64L.xml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The code below trains a LambdaMART of 1000 trees.\n",
    "\n",
    "!{QUICKRANK} \\\n",
    "  --algo LAMBDAMART \\\n",
    "  --num-trees 1000 \\\n",
    "  --shrinkage 0.05 \\\n",
    "  --num-thresholds 0 \\\n",
    "  --num-leaves 64 \\\n",
    "  --min-leaf-support 1 \\\n",
    "  --end-after-rounds 0 \\\n",
    "  --partial 1000 \\\n",
    "  --train {train_dataset_file} \\\n",
    "  --valid {valid_dataset_file} \\\n",
    "  --train-metric NDCG \\\n",
    "  --train-cutoff 10 \\\n",
    "  --model-out ~/quickrank.1000T.64L.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "We now translate the LambdaMART model in C++ code employing Conditional Operators to build the final document ranker [QS-TOIS16].\n",
    "\n",
    "QuickRank provides a plugin to convert models stored in its native XML format to C++ source code. The result is that each tree is translated as a nested block of Conditional Operators (https://www.tutorialspoint.com/cplusplus/cpp_conditional_operator.htm). The obtained C++ code can be compiled to produce a working ranked of the given model.\n",
    "\n",
    "Here a toy example of a tree:\n",
    "~~~~\n",
    "<feature>194</feature>\n",
    "<threshold>140</threshold>\n",
    " <split pos=\"left\">\n",
    "   <feature>31</feature>\n",
    "   <threshold>0.0120639997</threshold>\n",
    "     <split pos=\"left\">\n",
    "       <output>-0.78920207999267233</output>\n",
    "     </split>\n",
    "     <split pos=\"right\">\n",
    "       <output>1.1050481952095461</output>\n",
    "     </split>\n",
    " </split>\n",
    "~~~~\n",
    "\n",
    "The conditional operator (BOOLEAN CONDITION ? THEN : ELSE) translation produces:\n",
    "\n",
    "~~~~\n",
    "v[194] <= 140.0f ? ( v[31] <= 0.0120639997f ? -0.78920207999267233 : 1.1050481952095461 )\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use CondOp-based C code as a baseline for the scoring time evaluation\n",
    "\n",
    "def run_condop(model_file, dataset_file, rounds=1):\n",
    "    # create the C code\n",
    "    print (\" 1. Creating the C code for \" + model_file)\n",
    "    condop_source = model_file + \".c\"\n",
    "    \n",
    "    _ = !{QUICKRANK} \\\n",
    "      --generator condop \\\n",
    "      --model-file {model_file} \\\n",
    "      --code-file {condop_source}\n",
    "    \n",
    "    # Compile an executable ranker. The resulting ranker is SCORER=./quickrank/bin/quickscore\n",
    "    print (\" 2. Compiling the model\")\n",
    "\n",
    "    # replace empty scorer\n",
    "    !cp {condop_source} ./quickrank/src/scoring/ranker.cc\n",
    "    # compile\n",
    "    _ = !make -j -C ./quickrank/build_ quickscore \n",
    "    \n",
    "    # Now running the Conditional Operators scorer by executing the previously compiled C code.\n",
    "    # QuickScore options:\n",
    "    #  -h,--help                             print help message\n",
    "    #  -d,--dataset <arg>                    Input dataset in SVML format\n",
    "    #  -r,--rounds <arg> (10)                Number of test repetitions\n",
    "    #  -s,--scores <arg>                     File where scores are saved (Optional).\n",
    "    print (\" 3. Running the compiled model\")\n",
    "    cond_op_scorer_out = !{SCORER} \\\n",
    "      -d {dataset_file} \\\n",
    "      -r {rounds}\n",
    "    \n",
    "    print (cond_op_scorer_out.n)\n",
    "    \n",
    "    # takes the scoring time in milli-seconds\n",
    "    cond_op_scoring_time = float(cond_op_scorer_out.l[-1].split()[-2])* 10**6\n",
    "    \n",
    "    return cond_op_scoring_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Creating the C code for models/istella-small.lamdamart.xml\n",
      " 2. Compiling the model\n",
      " 3. Running the compiled model\n",
      "\n",
      "      _____  _____\n",
      "     /    / /____/\n",
      "    /____\\ /    \\          QuickRank has been developed by hpc.isti.cnr.it\n",
      "    ::Quick:Rank::                                   quickrank@isti.cnr.it\n",
      "\n",
      "#\t Dataset size: 681250 x 220 (instances x features)\n",
      "#\t Num queries: 6562 | Avg. len: 104\n",
      "       Total scoring time: 73.6 s.\n",
      "Avg. Dataset scoring time: 73.6 s.\n",
      "Avg.    Doc. scoring time: 0.000108 s.\n"
     ]
    }
   ],
   "source": [
    "condop_efficiency = run_condop(baseline_model_file, test_dataset_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th># Trees</th>\n",
       "      <th>Scoring Time µs.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CondOp</td>\n",
       "      <td>1492</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model # Trees  Scoring Time µs.\n",
       "0  CondOp    1492             108.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store current results\n",
    "results = pd.DataFrame(columns=['Model', '# Trees', 'Scoring Time µs.'])\n",
    "\n",
    "results.loc[len(results)] = ['CondOp', baseline_model.n_trees, condop_efficiency]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Now scoring with VPred [VPRED].\n",
    "\n",
    "First of all, we need to convert the QuickRank XML model in the VPRED format. Finally, use it to score the test file.\n",
    "\n",
    "QuickRank provides a plugin to convert models stored in its native XML format to the textual representation employed by the original VPRED code by Nima Asadi et al. [VPRED]. The plugin outputs a textual file. \n",
    " \n",
    "The original VPred code and instructions on how to compile, install and use it are available here: https://github.com/lintool/OptTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m\r\n",
      "      _____  _____\r\n",
      "     /    / /____/\r\n",
      "    /____\\ /    \\        QuickRank has been developed by hpc.isti.cnr.it\r\n",
      "    ::Quick:Rank::                             mail: quickrank@isti.cnr.it\r\n",
      "\u001b[0m\r\n",
      "generating VPred input file from: models/istella-small.lamdamart.xml\r\n"
     ]
    }
   ],
   "source": [
    "vpred_source = baseline_model_file + \".vpred\"\n",
    "\n",
    "!{QUICKRANK} \\\n",
    "  --generator vpred \\\n",
    "  --model-file {baseline_model_file} \\\n",
    "  --code-file {vpred_source}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\t0.000102213185\n",
      "Ignore this number: -2779350\n"
     ]
    }
   ],
   "source": [
    "# Now running the VPred scorer by using the previously converted code.\n",
    "# note that we are using the original VPred code by Asadi et al. [VPRED].\n",
    "# The code is available here: https://github.com/lintool/OptTrees\n",
    "\n",
    "vpred_scorer_out = !{VPRED} \\\n",
    "  -ensemble {vpred_source} \\\n",
    "  -instances {vpred_test_dataset_file} \\\n",
    "  -maxLeaves 64\n",
    "    \n",
    "print (vpred_scorer_out.n)\n",
    "\n",
    "# takes the scoring time in milli-seconds\n",
    "vpred_scoring_time = float(vpred_scorer_out.l[0].split('\\t')[1])* 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th># Trees</th>\n",
       "      <th>Scoring Time µs.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CondOp</td>\n",
       "      <td>1492</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VPred</td>\n",
       "      <td>1492</td>\n",
       "      <td>102.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model # Trees  Scoring Time µs.\n",
       "0  CondOp    1492             108.0\n",
       "1   VPred    1492             102.2"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store current results\n",
    "results.loc[len(results)] = ['VPred', baseline_model.n_trees, vpred_scoring_time]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "QuickScorer uses a novel traversal methods and a cache-friendly data layout that reduces dramatically the traversal time [QS-SIGIR15, QS-TOIS16]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      _____  _____\n",
      "     /    / /____\n",
      "    /____\\ _____/          QuickScorer has been developed by hpc.isti.cnr.it\n",
      "    :Quick:Scorer:                                   quickscorer@isti.cnr.it\n",
      "\n",
      "#\t Dataset size: 681250 x 220 (instances x features)\n",
      "#\t Num queries: 6562 | Avg. len: 104\n",
      "       Total scoring time: 15.7592 s.\n",
      "Avg. Dataset scoring time: 15.7592 s.\n",
      "Avg.    Doc. scoring time: 2.31327e-05 s.\n"
     ]
    }
   ],
   "source": [
    "# Now running QuickScorer.\n",
    "# note that we are using the original QuickScorer code by Lucchese et al. [QS-SIGIR15,QS-TOIS16].\n",
    "# The code is available under NDA.\n",
    "#\n",
    "# Options:\n",
    "#  -h [ --help ]                     Print help messages.\n",
    "#  -d [ --dataset ] arg              Path of the dataset to score (SVML format).\n",
    "#  -r [ --rounds ] arg (=10)         Number of test repetitions.\n",
    "#  -s [ --scores ] arg               Path of the file where final scores are\n",
    "#                                    saved.\n",
    "#  -t [ --tree_type ] arg (=0)       Specify the type of the tree in the\n",
    "#                                    ensemble:\n",
    "#                                     - 0 for normal trees,\n",
    "#                                     - 1 for oblivious trees,\n",
    "#                                     - 2 for normal trees (reversed blocked),\n",
    "#                                     - 3 for normal trees (SIMD: SSE/AVX).\n",
    "#  -m [ --model ] arg                Path of the XML file storing the model.\n",
    "#  -l [ --nleaves ] arg              Maximum number of leaves in a tree (<= 64).\n",
    "#  --avx                             Use AVX 256 instructions (at least 8 doc\n",
    "#                                    blocking).\n",
    "#  --omp                             Use OpenMP multi-threading document scoring\n",
    "#                                    (only SIMD: SSE/AVX).\n",
    "\n",
    "qs_scorer_out = !{QUICKSCORER} \\\n",
    "  -d {test_dataset_file} \\\n",
    "  -m {baseline_model_file} \\\n",
    "  -l 64 \\\n",
    "  -r 1 \\\n",
    "  -t 0\n",
    "    \n",
    "print (qs_scorer_out.n)\n",
    "    \n",
    "# takes the scoring time in milli-seconds\n",
    "qs_scoring_time = float(qs_scorer_out.l[-1].split()[-2])* 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th># Trees</th>\n",
       "      <th>Scoring Time µs.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CondOp</td>\n",
       "      <td>1492</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VPred</td>\n",
       "      <td>1492</td>\n",
       "      <td>102.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QS</td>\n",
       "      <td>1492</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model # Trees  Scoring Time µs.\n",
       "0  CondOp    1492             108.0\n",
       "1   VPred    1492             102.2\n",
       "2      QS    1492              23.1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store current results\n",
    "results.loc[len(results)] = ['QS', baseline_model.n_trees, qs_scoring_time]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Vectorized QuickScorer improves over QuickScorer by exploiting 256-bits wide CPU registers [QS-SIGIR16].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      _____  _____\n",
      "     /    / /____\n",
      "    /____\\ _____/          QuickScorer has been developed by hpc.isti.cnr.it\n",
      "    :Quick:Scorer:                                   quickscorer@isti.cnr.it\n",
      "\n",
      "#\t Dataset size: 681250 x 220 (instances x features)\n",
      "#\t Num queries: 6562 | Avg. len: 104\n",
      "       Total scoring time: 10.4056 s.\n",
      "Avg. Dataset scoring time: 10.4056 s.\n",
      "Avg.    Doc. scoring time: 1.52742e-05 s.\n"
     ]
    }
   ],
   "source": [
    "# Now running Vectorized QuickScorer (AVX2)\n",
    "# note that we are using the original QuickScorer code by Lucchese et al. [QS-SIGIR16].\n",
    "# The code is available under NDA.\n",
    "#\n",
    "# Options:\n",
    "#  -h [ --help ]                     Print help messages.\n",
    "#  -d [ --dataset ] arg              Path of the dataset to score (SVML format).\n",
    "#  -r [ --rounds ] arg (=10)         Number of test repetitions.\n",
    "#  -s [ --scores ] arg               Path of the file where final scores are\n",
    "#                                    saved.\n",
    "#  -t [ --tree_type ] arg (=0)       Specify the type of the tree in the\n",
    "#                                    ensemble:\n",
    "#                                     - 0 for normal trees,\n",
    "#                                     - 1 for oblivious trees,\n",
    "#                                     - 2 for normal trees (reversed blocked),\n",
    "#                                     - 3 for normal trees (SIMD: SSE/AVX).\n",
    "#  -m [ --model ] arg                Path of the XML file storing the model.\n",
    "#  -l [ --nleaves ] arg              Maximum number of leaves in a tree (<= 64).\n",
    "#  -v [ --doc_block_size ] arg (=1)  Document block size (allowed values:\n",
    "#                                    1,2,4,8,16; 1 means no blocking).\n",
    "#  --avx                             Use AVX 256 instructions (at least 8 doc\n",
    "#                                    blocking).\n",
    "#  --omp                             Use OpenMP multi-threading document scoring\n",
    "#                                    (only SIMD: SSE/AVX).\n",
    "\n",
    "vqs_scorer_out = !{QUICKSCORER} \\\n",
    "  -d {test_dataset_file} \\\n",
    "  -m {baseline_model_file} \\\n",
    "  -l 64 \\\n",
    "  -r 1 \\\n",
    "  -t 3 \\\n",
    "  -v 8 \\\n",
    "  --avx\n",
    "    \n",
    "print (vqs_scorer_out.n)\n",
    "    \n",
    "# takes the scoring time in milli-seconds\n",
    "vqs_scoring_time = float(vqs_scorer_out.l[-1].split()[-2])* 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th># Trees</th>\n",
       "      <th>Scoring Time µs.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CondOp</td>\n",
       "      <td>1492</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VPred</td>\n",
       "      <td>1492</td>\n",
       "      <td>102.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QS</td>\n",
       "      <td>1492</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v-QS</td>\n",
       "      <td>1492</td>\n",
       "      <td>15.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model # Trees  Scoring Time µs.\n",
       "0  CondOp    1492             108.0\n",
       "1   VPred    1492             102.2\n",
       "2      QS    1492              23.1\n",
       "3    v-QS    1492              15.3"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store current results\n",
    "results.loc[len(results)] = ['v-QS', baseline_model.n_trees, vqs_scoring_time]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [QS-TOIS16]\n",
    "\n",
    "Some considerations:\n",
    "\n",
    "0. We reproduce the evaluation methodology presented in [QS-SIGIR15, QS-TOIS16] on a different LambdaMART. The LambdaMART here is composed of 1,492 trees while results in the two papers above are for 1,000 or 5,000 trees.\n",
    "0. The scoring time is 1.5x higher than the results reported in [QS-TOIS16] for all methods. This because we are running these experiments on a slower machine than the one used for producing the experimental results presented in [QS-SIGIR15, QS-TOIS16].\n",
    "\n",
    "![caption](images/HO1-scoring.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 6\n",
    "\n",
    "Low-level statistics of the scorer with ```perf```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**perf** (https://perf.wiki.kernel.org/index.php/Tutorial) is a profiler tool for Linux 2.6+ based systems that abstracts away CPU hardware differences in Linux performance measurements and presents a simple commandline interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.1\n",
    "\n",
    "```perf``` on QuickScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      _____  _____\n",
      "     /    / /____\n",
      "    /____\\ _____/          QuickScorer has been developed by hpc.isti.cnr.it\n",
      "    :Quick:Scorer:                                   quickscorer@isti.cnr.it\n",
      "\n",
      "#\t Dataset size: 681250 x 220 (instances x features)\n",
      "#\t Num queries: 6562 | Avg. len: 104\n",
      "       Total scoring time: 15.8269 s.\n",
      "Avg. Dataset scoring time: 15.8269 s.\n",
      "Avg.    Doc. scoring time: 2.32322e-05 s.\n",
      "\n",
      " Performance counter stats for './QuickScorer/bin/quickscorer -d /data/letor-datasets/tiscali/sample/test.txt -m models/istella-small.lamdamart.xml -l 64 -r 1 -t 0':\n",
      "\n",
      "   107,441,099,482 L1-dcache-loads                                              [36.36%]\n",
      "     5,768,612,463 L1-dcache-load-misses     #    5.37% of all L1-dcache hits   [36.36%]\n",
      "    55,253,754,540 L1-dcache-stores                                             [36.36%]\n",
      "       631,099,138 L1-dcache-store-misses                                       [36.37%]\n",
      "   <not supported> L1-icache-loads         \n",
      "        31,893,046 L1-icache-load-misses     #    0.00% of all L1-icache hits   [36.38%]\n",
      "   338,108,357,318 instructions              #    2.15  insns per cycle         [45.47%]\n",
      "   157,467,771,381 cycles                    [45.47%]\n",
      "     2,236,116,037 cache-references                                             [45.46%]\n",
      "        38,126,482 cache-misses              #    1.705 % of all cache refs     [45.46%]\n",
      "    70,617,883,858 branches                                                     [45.46%]\n",
      "       431,218,606 branch-misses             #    0.61% of all branches         [36.36%]\n",
      "\n",
      "      49.367583637 seconds time elapsed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below, perf is used to monitor several behaviours of the scorer:\n",
    "# - L1 cache performance (references and misses): L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,L1-dcache-store-misses,L1-icache-loads,L1-icache-load-misses\n",
    "# - L3 cache performance (references and misses): cache-references,cache-misses\n",
    "# - number of instructions and cycles: instructions,cycles\n",
    "# - total number of branches and branch misprediction: branches,branch-misses\n",
    "\n",
    "perf_out = !{PERF} stat -e \\\n",
    "  L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,\\\n",
    "L1-dcache-store-misses,L1-icache-loads,L1-icache-load-misses,\\\n",
    "instructions,cycles,cache-references,cache-misses,branches,branch-misses\\\n",
    "    {QUICKSCORER} \\\n",
    "      -d {test_dataset_file} \\\n",
    "      -m {baseline_model_file} \\\n",
    "      -l 64 \\\n",
    "      -r 1 \\\n",
    "      -t 0\n",
    "\n",
    "print (perf_out.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parsing perf output\n",
    "num_istructions = int(perf_out[20].strip().split(' ')[0].replace(',', ''))\n",
    "num_cache_ref = int(perf_out[22].strip().split(' ')[0].replace(',', ''))\n",
    "num_cache_miss = int(perf_out[23].strip().split(' ')[0].replace(',', ''))\n",
    "num_branches = int(perf_out[24].strip().split(' ')[0].replace(',', ''))\n",
    "num_branch_misses = int(perf_out[25].strip().split(' ')[0].replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2\n",
    "\n",
    "```perf``` on QuickScorer (no scoring)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      _____  _____\n",
      "     /    / /____\n",
      "    /____\\ _____/          QuickScorer has been developed by hpc.isti.cnr.it\n",
      "    :Quick:Scorer:                                   quickscorer@isti.cnr.it\n",
      "\n",
      "#\t Dataset size: 681250 x 220 (instances x features)\n",
      "#\t Num queries: 6562 | Avg. len: 104\n",
      "       Total scoring time: 7.7e-08 s.\n",
      "Avg. Dataset scoring time: 7.7e-08 s.\n",
      "Avg.    Doc. scoring time: 1.13028e-13 s.\n",
      "\n",
      " Performance counter stats for './QuickScorer-noscoring/bin/quickscorer -d /data/letor-datasets/tiscali/sample/test.txt -m models/istella-small.lamdamart.xml -l 64 -r 1 -t 0':\n",
      "\n",
      "    54,580,005,206 L1-dcache-loads                                              [36.37%]\n",
      "       172,014,442 L1-dcache-load-misses     #    0.32% of all L1-dcache hits   [36.39%]\n",
      "    38,995,829,137 L1-dcache-stores                                             [36.40%]\n",
      "        99,944,662 L1-dcache-store-misses                                       [36.40%]\n",
      "   <not supported> L1-icache-loads         \n",
      "        26,027,639 L1-icache-load-misses     #    0.00% of all L1-icache hits   [36.40%]\n",
      "   259,773,604,958 instructions              #    2.42  insns per cycle         [45.49%]\n",
      "   107,166,012,489 cycles                    [45.47%]\n",
      "        77,742,746 cache-references                                             [45.46%]\n",
      "        36,323,802 cache-misses              #   46.723 % of all cache refs     [45.45%]\n",
      "    61,594,774,071 branches                                                     [45.43%]\n",
      "       313,937,858 branch-misses             #    0.51% of all branches         [36.33%]\n",
      "\n",
      "      33.568790752 seconds time elapsed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below, perf is used to monitor several behaviours of the scorer:\n",
    "# - L1 cache performance (references and misses): L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,L1-dcache-store-misses,L1-icache-loads,L1-icache-load-misses\n",
    "# - L3 cache performance (references and misses): cache-references,cache-misses\n",
    "# - number of instructions and cycles: instructions,cycles\n",
    "# - total number of branches and branch misprediction: branches,branch-misses\n",
    "\n",
    "perf_noscoring_out = !{PERF} stat -e \\\n",
    "  L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,\\\n",
    "L1-dcache-store-misses,L1-icache-loads,L1-icache-load-misses,\\\n",
    "instructions,cycles,cache-references,cache-misses,branches,branch-misses\\\n",
    "    {QUICKSCORER_NS} \\\n",
    "      -d {test_dataset_file} \\\n",
    "      -m {baseline_model_file} \\\n",
    "      -l 64 \\\n",
    "      -r 1 \\\n",
    "      -t 0\n",
    "        \n",
    "print (perf_noscoring_out.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parsing perf output\n",
    "num_istructions_ns = int(perf_noscoring_out[20].strip().split(' ')[0].replace(',', ''))\n",
    "num_cache_ref_ns = int(perf_noscoring_out[22].strip().split(' ')[0].replace(',', ''))\n",
    "num_cache_miss_ns = int(perf_noscoring_out[23].strip().split(' ')[0].replace(',', ''))\n",
    "num_branches_ns = int(perf_noscoring_out[24].strip().split(' ')[0].replace(',', ''))\n",
    "num_branch_misses_ns = int(perf_noscoring_out[25].strip().split(' ')[0].replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.3\n",
    "\n",
    "now computing differences between the two runs to get the low level statistics for the scoring part of QS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Cache Misses</th>\n",
       "      <th>Branch Misprediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QS</td>\n",
       "      <td>77.1</td>\n",
       "      <td>1.8e-03</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Method  Instructions  Cache Misses  Branch Misprediction\n",
       "0     QS          77.1       1.8e-03                   0.1"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store current results\n",
    "perf_results = pd.DataFrame(columns=['Method', 'Instructions', 'Cache Misses', 'Branch Misprediction'])\n",
    "\n",
    "normalized_instruction_count = (num_istructions - num_istructions_ns) / float(dataset_size * baseline_model.n_trees)\n",
    "normalized_cache_miss = (num_cache_miss - num_cache_miss_ns) / float(dataset_size * baseline_model.n_trees)\n",
    "normalized_branch_miss = (num_branch_misses - num_branch_misses_ns) / float(dataset_size * baseline_model.n_trees)\n",
    "\n",
    "perf_results.loc[len(perf_results)] = ['QS',\n",
    "                                  normalized_instruction_count,\n",
    "                                  normalized_cache_miss,\n",
    "                                  normalized_branch_miss]\n",
    "perf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.4\n",
    "\n",
    "The same methodology now on VPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\t0.0001024996\n",
      "Ignore this number: -2779350\n",
      "\n",
      " Performance counter stats for './asadi_tkde13/out/VPred -ensemble models/istella-small.lamdamart.xml.vpred -instances /data/letor-datasets/tiscali/sample/test.vpred -maxLeaves 64':\n",
      "\n",
      "   265,219,690,462 L1-dcache-loads                                              [36.36%]\n",
      "     1,396,690,892 L1-dcache-load-misses     #    0.53% of all L1-dcache hits   [36.36%]\n",
      "    89,900,989,195 L1-dcache-stores                                             [36.36%]\n",
      "        24,883,290 L1-dcache-store-misses                                       [36.36%]\n",
      "   <not supported> L1-icache-loads         \n",
      "     9,354,194,157 L1-icache-load-misses     #    0.00% of all L1-icache hits   [36.37%]\n",
      "   625,127,341,692 instructions              #    2.09  insns per cycle         [45.46%]\n",
      "   298,491,704,130 cycles                    [45.46%]\n",
      "    15,796,189,517 cache-references                                             [45.46%]\n",
      "        10,809,889 cache-misses              #    0.068 % of all cache refs     [45.46%]\n",
      "    41,062,857,428 branches                                                     [45.46%]\n",
      "       196,418,165 branch-misses             #    0.48% of all branches         [36.37%]\n",
      "\n",
      "      93.504234616 seconds time elapsed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below, perf is used to monitor several behaviours of the scorer:\n",
    "# - L1 cache performance (references and misses): L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,L1-dcache-store-misses,L1-icache-loads,L1-icache-load-misses\n",
    "# - L3 cache performance (references and misses): cache-references,cache-misses\n",
    "# - number of instructions and cycles: instructions,cycles\n",
    "# - total number of branches and branch misprediction: branches,branch-misses\n",
    "\n",
    "vpred_perf_out = !{PERF} stat -e \\\n",
    "  L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,\\\n",
    "L1-dcache-store-misses,L1-icache-loads,L1-icache-load-misses,\\\n",
    "instructions,cycles,cache-references,cache-misses,branches,branch-misses\\\n",
    "    {VPRED} \\\n",
    "      -ensemble {vpred_source} \\\n",
    "      -instances {vpred_test_dataset_file} \\\n",
    "      -maxLeaves 64\n",
    "        \n",
    "print (vpred_perf_out.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parsing perf output\n",
    "num_istructions = int(vpred_perf_out[11].strip().split(' ')[0].replace(',', ''))\n",
    "num_cache_ref = int(vpred_perf_out[13].strip().split(' ')[0].replace(',', ''))\n",
    "num_cache_miss = int(vpred_perf_out[14].strip().split(' ')[0].replace(',', ''))\n",
    "num_branches = int(vpred_perf_out[15].strip().split(' ')[0].replace(',', ''))\n",
    "num_branch_misses = int(vpred_perf_out[16].strip().split(' ')[0].replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.5\n",
    "\n",
    "``perf`` on VPred (no scoring)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\t1.35045872e-13\n",
      "Ignore this number: 0\n",
      "\n",
      " Performance counter stats for './asadi_tkde13-noscoring/out/VPred -ensemble models/istella-small.lamdamart.xml.vpred -instances /data/letor-datasets/tiscali/sample/test.vpred -maxLeaves 64':\n",
      "\n",
      "    43,924,987,417 L1-dcache-loads                                              [36.34%]\n",
      "        39,034,171 L1-dcache-load-misses     #    0.09% of all L1-dcache hits   [36.35%]\n",
      "    31,210,751,758 L1-dcache-stores                                             [36.40%]\n",
      "        19,913,536 L1-dcache-store-misses                                       [36.41%]\n",
      "   <not supported> L1-icache-loads         \n",
      "         3,333,337 L1-icache-load-misses     #    0.00% of all L1-icache hits   [36.40%]\n",
      "   175,364,020,154 instructions              #    2.32  insns per cycle         [45.49%]\n",
      "    75,705,977,800 cycles                    [45.48%]\n",
      "        21,001,104 cache-references                                             [45.48%]\n",
      "         7,073,744 cache-misses              #   33.683 % of all cache refs     [45.46%]\n",
      "    40,852,893,412 branches                                                     [45.45%]\n",
      "       152,470,709 branch-misses             #    0.37% of all branches         [36.34%]\n",
      "\n",
      "      23.969216571 seconds time elapsed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below, perf is used to monitor several behaviours of the scorer:\n",
    "# - L1 cache performance (references and misses): L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,L1-dcache-store-misses,L1-icache-loads,L1-icache-load-misses\n",
    "# - L3 cache performance (references and misses): cache-references,cache-misses\n",
    "# - number of instructions and cycles: instructions,cycles\n",
    "# - total number of branches and branch misprediction: branches,branch-misses\n",
    "\n",
    "vpred_perf_noscoring_out = !{PERF} stat -e \\\n",
    "  L1-dcache-loads,L1-dcache-load-misses,L1-dcache-stores,\\\n",
    "L1-dcache-store-misses,L1-icache-loads,L1-icache-load-misses,\\\n",
    "instructions,cycles,cache-references,cache-misses,branches,branch-misses\\\n",
    "    {VPRED_NS} \\\n",
    "      -ensemble {vpred_source} \\\n",
    "      -instances {vpred_test_dataset_file} \\\n",
    "      -maxLeaves 64\n",
    "        \n",
    "print (vpred_perf_noscoring_out.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parsing perf output\n",
    "num_istructions_ns = int(vpred_perf_noscoring_out[11].strip().split(' ')[0].replace(',', ''))\n",
    "num_cache_ref_ns = int(vpred_perf_noscoring_out[13].strip().split(' ')[0].replace(',', ''))\n",
    "num_cache_miss_ns = int(vpred_perf_noscoring_out[14].strip().split(' ')[0].replace(',', ''))\n",
    "num_branches_ns = int(vpred_perf_noscoring_out[15].strip().split(' ')[0].replace(',', ''))\n",
    "num_branch_misses_ns = int(vpred_perf_noscoring_out[16].strip().split(' ')[0].replace(',', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.6\n",
    "\n",
    "now computing differences between the two runs to get the low level statistics for the scoring part of VPred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Cache Misses</th>\n",
       "      <th>Branch Misprediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QS</td>\n",
       "      <td>77.1</td>\n",
       "      <td>1.8e-03</td>\n",
       "      <td>1.2e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VPred</td>\n",
       "      <td>442.5</td>\n",
       "      <td>3.7e-03</td>\n",
       "      <td>4.3e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Method  Instructions  Cache Misses  Branch Misprediction\n",
       "0     QS          77.1       1.8e-03               1.2e-01\n",
       "1  VPred         442.5       3.7e-03               4.3e-02"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store current results\n",
    "normalized_instruction_count = (num_istructions - num_istructions_ns) / float(dataset_size * baseline_model.n_trees)\n",
    "normalized_cache_miss = (num_cache_miss - num_cache_miss_ns) / float(dataset_size * baseline_model.n_trees)\n",
    "normalized_branch_miss = (num_branch_misses - num_branch_misses_ns) / float(dataset_size * baseline_model.n_trees)\n",
    "\n",
    "perf_results.loc[len(perf_results)] = ['VPred',\n",
    "                                  normalized_instruction_count,\n",
    "                                  normalized_cache_miss,\n",
    "                                  normalized_branch_miss]\n",
    "perf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.7\n",
    "\n",
    "from [QS-TOIS16]\n",
    "\n",
    "Some considerations:\n",
    "\n",
    "0. We reproduce the methodology presented in [QS-SIGIR15, QS-TOIS16] on a different LambdaMART. The LambdaMART here is composed of 1,492 trees while results in the two papers above are for 1,000 or 5,000 trees. Given that said, the low level behavior of the two methods is confirmed.\n",
    "0. The number of instructions executed by VPred is the largest one. This is because VPred always runs ``d`` steps, where ``d`` is the depth of a tree even if a document might reach an exit leaf earlier. On the other hand, QS executes the smallest number instructions. This is due to the different traversal strategy of the ensemble, as QS needs to process the false nodes only.\n",
    "0. In terms of number of branches, we note that QS has a larger total number of branch mispredictions than VPred, which uses scoring functions that are branch-free.\n",
    "0. In terms of cache misses, we note that QS has a lower cache miss. This is mostly due to the new data layout of QS that perform document scoring by means of linear scans of arrays.\n",
    "\n",
    "![caption](images/HO1-lowlevelperf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bonus Track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "\n",
    "We prepare a Multithreaded implementation of Vectorized QuickScorer that exploits OpenMP to distribute bunches of documents to threads scoring them in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting up environment variables for OpenMP\n",
    "os.environ['OMP_NUM_THREADS']='32'\n",
    "os.environ['OMP_DISPLAY_ENV']='VERBOSE'\n",
    "os.environ['OMP_SCHEDULE']='auto'\n",
    "os.environ['GOMP_CPU_AFFINITY']='0-7,8-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OPENMP DISPLAY ENVIRONMENT BEGIN\n",
      "  _OPENMP = '201511'\n",
      "  OMP_DYNAMIC = 'FALSE'\n",
      "  OMP_NESTED = 'FALSE'\n",
      "  OMP_NUM_THREADS = '32'\n",
      "  OMP_SCHEDULE = 'AUTO'\n",
      "  OMP_PROC_BIND = 'TRUE'\n",
      "  OMP_PLACES = '{0},{1},{2},{3},{4},{5},{6},{7},{8},{9},{10},{11},{12},{13},{14},{15}'\n",
      "  OMP_STACKSIZE = '0'\n",
      "  OMP_WAIT_POLICY = 'PASSIVE'\n",
      "  OMP_THREAD_LIMIT = '4294967295'\n",
      "  OMP_MAX_ACTIVE_LEVELS = '2147483647'\n",
      "  OMP_CANCELLATION = 'FALSE'\n",
      "  OMP_DEFAULT_DEVICE = '0'\n",
      "  OMP_MAX_TASK_PRIORITY = '0'\n",
      "  GOMP_CPU_AFFINITY = ''\n",
      "  GOMP_STACKSIZE = '0'\n",
      "  GOMP_SPINCOUNT = '300000'\n",
      "OPENMP DISPLAY ENVIRONMENT END\n",
      "\n",
      "      _____  _____\n",
      "     /    / /____\n",
      "    /____\\ _____/          QuickScorer has been developed by hpc.isti.cnr.it\n",
      "    :Quick:Scorer:                                   quickscorer@isti.cnr.it\n",
      "\n",
      "#\t Dataset size: 681250 x 220 (instances x features)\n",
      "#\t Num queries: 6562 | Avg. len: 104\n",
      "       Total scoring time: 0.845968 s.\n",
      "Avg. Dataset scoring time: 0.845968 s.\n",
      "Avg.    Doc. scoring time: 1.24179e-06 s.\n"
     ]
    }
   ],
   "source": [
    "# Now running Multi-threaded Vectorized QuickScorer.\n",
    "# Options:\n",
    "#  -h [ --help ]                     Print help messages.\n",
    "#  -d [ --dataset ] arg              Path of the dataset to score (SVML format).\n",
    "#  -r [ --rounds ] arg (=10)         Number of test repetitions.\n",
    "#  -s [ --scores ] arg               Path of the file where final scores are\n",
    "#                                    saved.\n",
    "#  -t [ --tree_type ] arg (=0)       Specify the type of the tree in the\n",
    "#                                    ensemble:\n",
    "#                                     - 0 for normal trees,\n",
    "#                                     - 1 for oblivious trees,\n",
    "#                                     - 2 for normal trees (reversed blocked),\n",
    "#                                     - 3 for normal trees (SIMD: SSE/AVX).\n",
    "#  -m [ --model ] arg                Path of the XML file storing the model.\n",
    "#  -l [ --nleaves ] arg              Maximum number of leaves in a tree (<= 64).\n",
    "#  -v [ --doc_block_size ] arg (=1)  Document block size (allowed values:\n",
    "#                                    1,2,4,8,16; 1 means no blocking).\n",
    "#  --avx                             Use AVX 256 instructions (at least 8 doc\n",
    "#                                    blocking).\n",
    "#  --omp                             Use OpenMP multi-threading document scoring\n",
    "#                                    (only SIMD: SSE/AVX).\n",
    "\n",
    "scorer_out = !{QUICKSCORER} \\\n",
    "  -d {test_dataset_file} \\\n",
    "  -m {baseline_model_file} \\\n",
    "  -l 64 \\\n",
    "  -r 1 \\\n",
    "  -t 3 \\\n",
    "  -v 8 \\\n",
    "  --avx \\\n",
    "  --omp\n",
    "    \n",
    "print (scorer_out.n)\n",
    "    \n",
    "# takes the scoring time in milli-seconds\n",
    "scoring_time = float(scorer_out.l[-1].split()[-2])* 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th># Trees</th>\n",
       "      <th>Scoring Time µs.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CondOp</td>\n",
       "      <td>1492</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VPred</td>\n",
       "      <td>1492</td>\n",
       "      <td>102.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QS</td>\n",
       "      <td>1492</td>\n",
       "      <td>23.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v-QS</td>\n",
       "      <td>1492</td>\n",
       "      <td>15.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vQS-OMP</td>\n",
       "      <td>1492</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model # Trees  Scoring Time µs.\n",
       "0   CondOp    1492             108.0\n",
       "1    VPred    1492             102.2\n",
       "2       QS    1492              23.1\n",
       "3     v-QS    1492              15.3\n",
       "4  vQS-OMP    1492               1.2"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store current results\n",
    "results.loc[len(results)] = ['vQS-OMP', baseline_model.n_trees, scoring_time]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "\n",
    "RankEval (http://rankeval.isti.cnr.it) - multithread scoring written in Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files. This may take a few minutes.\n",
      "done loading dataset!\n"
     ]
    }
   ],
   "source": [
    "from rankeval.dataset.datasets_fetcher import load_dataset\n",
    "\n",
    "dataset_container = load_dataset(dataset_name='istella-sample',\n",
    "                                download_if_missing=True, \n",
    "                                force_download=False, \n",
    "                                with_models=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 39840.24 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 3: 263 µs per loop\n"
     ]
    }
   ],
   "source": [
    "# We now use RankEval to score the test file.\n",
    "scorer_out = %timeit -o baseline_model.score(dataset_container.test_dataset, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[VPRED] Asadi et al. Runtime Optimizations for Tree-Based Machine Learning Models. IEEE Trans. Knowl. Data Eng. 26(9): 2281-2292 (2014).\n",
    "\n",
    "[QS-SIGIR15] Lucchese et al. QuickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees. ACM SIGIR 2015. Best Paper Award.\n",
    "\n",
    "[QS-TOIS16] Dato et al. Fast Ranking with Additive Ensembles of Oblivious and Non-Oblivious Regression Trees. ACM TOIS, Vol. 9, No. 4. Dec. 2016.\n",
    "\n",
    "[QS-SIGIR16] Lucchese et al. Exploiting CPU SIMD Extensions to Speed-up Document Scoring with Tree Ensembles. ACM SIGIR 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
